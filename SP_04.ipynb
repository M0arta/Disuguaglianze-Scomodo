{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Python\n",
    "## Central European University\n",
    "\n",
    "## 04 Pandas, (Seaborn)\n",
    "\n",
    "Instructor: Márton Pósfai, TA: --\n",
    "\n",
    "Email: posfaim@ceu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Don't forget:* use the Slack channel for discussion, to ask questions, or to show solutions to exercises that are different from the ones provided in the notebook. [Slack channel](http://www.personal.ceu.edu/staff/Marton_Posfai/slack_forward.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas\n",
    "\n",
    "[Intro video](https://ceu.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=4aa0894d-686a-42d6-9f07-ab8a00d1e2bf)\n",
    "\n",
    "Introduce and explore [pandas](http://pandas.pydata.org/), a library for tabular data manipulating and analysis that has implementations of common tasks making the following (and more) very straightforward:\n",
    "- Reading in and cleaning tabular data\n",
    "- Merging data from different sources\n",
    "- Basic analysis and plotting\n",
    "\n",
    "Pandas stands for **Pan**el **Da**ta (an expression borrowed from econometrics), it brings tools and ideas from Excel, R, and SQL to Python.\n",
    "\n",
    "We'll have a look at the basic data structures: Series and Dataframes. Then we'll look at a dataset about the passengers of the Titanic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part I: Pandas and its data structures\n",
    "\n",
    "Traditionally everyone imports pandas as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# we will also use:\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Video.](https://ceu.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=0c046192-d9a5-4906-a275-acbd00fde38b)\n",
    "\n",
    "Pandas has two primary data structures: series and dataframes. Series are similar to Python lists or NumPy vectors: they are one dimensional. \n",
    "\n",
    "We can create a series from a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_list = [3,2,1,34,5,3,2,3,100]\n",
    "my_first_series = pd.Series(my_list)\n",
    "my_first_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we create the series, notice that we also get a column with a number for each row. This is the index of the series and it contains a label for each row. \n",
    "\n",
    "You can access the values and the indices separately: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S = my_first_series # just so I don't have to type that much\n",
    "\n",
    "#access the values\n",
    "print(S.values)\n",
    "print(type(S.values))\n",
    "\n",
    "#access indices\n",
    "print(S.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values are stored in a NumPy array.\n",
    "\n",
    "By default the indices are integers starting from 0. But you can use a variety of data types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = pd.Series([1.5,2.,3.],index=['A','B','C'])\n",
    "S1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2 = pd.Series([1,2,3])\n",
    "S2.index = [.5,.75,1.]\n",
    "S2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing elements\n",
    "\n",
    "There are three main ways access elements:\n",
    "* By index using `S.loc[...]`\n",
    "* By position using `S.iloc[...]`\n",
    "* And using `S[]`\n",
    "Let's look at these individually.\n",
    "\n",
    "#### `loc`\n",
    "With `S.loc[label]` you can access elements of `S` using the labels set as the index of the series, so this is kind of like indexing a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S1.loc['A'], S2.loc[.75], S.loc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike dictionaries, we can also use lists of labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S1.loc[['A','C']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels do not have to be in the original order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S2.loc[[1.,.5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "S.loc[[3,2,1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in this last example the numbers are the labels, which just happen to be the same as the position of the rows in the original series `S`.\n",
    "\n",
    "And you can also do slicing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "S1.loc['B':'C'] # if using labels, the end label 'C' is also included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `iloc`\n",
    "\n",
    "With `S.iloc[pos]` you can access elements based on their position, so this is kind of like the indexing of lists and arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S.iloc[0],S1.iloc[1],S2.iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again you can provide a list of positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S1.iloc[[2,1,0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or you can use slices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S2.iloc[0:2] # when using positions the final position, here 2, is not included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `[...]`\n",
    "\n",
    "So what does `S[x]` return? Let's try it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(S1['A'],S1[0])\n",
    "print(S2[.75],S1[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas guesses if it is a label or a position! But what happens both labels and positions are integers? Let's try it out, first let's create a series that has the position and label are both integers but in different order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S3 = S[::-1] #with this slice we reversed the order of rows\n",
    "S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(S3[0],S3[8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is based on label!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S3[1:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusingly, slices are based on position.\n",
    "\n",
    "Bottomline: when in doubt us `loc` and `iloc`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a series with values that are integers from 1 to 5, and index labels that are the first five letters of the English alphabet. Then select every second row, try to do it as many ways as you can think of!\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "Try using slices and list of both positions and labels.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "\n",
    "S = pd.Series(np.arange(1,6),index=['a','b','c','d','e'])\n",
    "print(S[::2])\n",
    "print(S.loc['a':'e':2])\n",
    "print(S.loc[['a','c','e']])\n",
    "print(S.iloc[[ i for i in range(0,5,2)]])\n",
    "\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Masking\n",
    "\n",
    "[Video.](https://ceu.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=1e5bae96-d184-413a-a633-acbd01020c36)\n",
    "\n",
    "We can also use masks the same way we did with NumPy arrays. A mask is a list or array of bools that has the same length as your series. Using it as an index will return elements where the mask is `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = pd.Series([3,5,2,3],index=['A','B','C','D'])\n",
    "mask = [True, False, True, False]\n",
    "S[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this to filter rows based on the value of their elements, by create a mask using comparison operators such as `==`, `>`, or `>=`. For example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "just_threes_mask = (S == 3)\n",
    "print(just_threes_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this mask to filter rows that are equal to 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "S[just_threes_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S[S==3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Using masking, select rows of the following series such that values are larger than 5 but smaller than 10. You can use a list comprehension to create the mask, or even better try to use the elementwise logic and operator `&`: \n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "You can iterate over the values of a series in either of the following two ways:\n",
    "```python\n",
    "    \n",
    "for x in S.values:\n",
    "    ...\n",
    "    \n",
    "for x in S:\n",
    "    ...\n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = pd.Series(range(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "print(S[(S>5) & (S<10)])\n",
    "\n",
    "mask = [x>5 and x<10 for x in S]\n",
    "print(S[mask])\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying functions to series\n",
    "\n",
    "[Video](https://ceu.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=56629303-7ec5-491a-b6ed-acbd010662fb)\n",
    "\n",
    "In most use cases, our series will contain only one data type, so let's look at examples like that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.Series([1.0, 2.0, 3.0])\n",
    "b = pd.Series([1.0, 1.0, 1.0])\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to numpy, basic operations get applied to the series elementwise. For example, multiplying a series by 2, doubles each element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2*a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or adding two series is calculated elementwise (elements are matched by label):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can even apply a numpy function to the enitre series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sin(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want apply a general function to the Series elemetwise we have to use the `apply` function. This is important, we will use it often to create new columns in tables.\n",
    "\n",
    "The `apply()` function for series is the same as list comprehensions are for lists. Let's look at a few examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [10,'apple',3.4,'hello']\n",
    "new_list = [ type(x) for x in L]\n",
    "print(new_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same using series and `apply()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = pd.Series(L)\n",
    "new_series = S.apply(type)\n",
    "print(new_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define our own functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list\n",
    "L = [\"apple!\",\"pear!\",\"watermelon!\"]\n",
    "L2 = [x.replace(\"!\",\"\") for x in L]\n",
    "print(L2)\n",
    "\n",
    "#series\n",
    "s = pd.Series(L)\n",
    "\n",
    "def func(x):\n",
    "    return x.replace(\"!\",\"\")\n",
    "\n",
    "s2 = s.apply(func)\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can do the same thing with lambda functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(L)\n",
    "s2 = s.apply(lambda x: x.replace(\"!\",\"\"))\n",
    "print(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Take the following list and list comprehension and\n",
    "* Convert the list into a pandas series\n",
    "* Use `apply()` to do the same operation as the list comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = [\"APPLE\",\"PEAR\",\"WATERMELON\"]\n",
    "L2 = [ x.lower()+\"!\" for x in L]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "L = [\"APPLE\",\"PEAR\",\"WATERMELON\"]\n",
    "L2 = [ x.lower()+\"!\" for x in L]\n",
    "\n",
    "s= pd.Series(L)\n",
    "s2 = s.apply(lambda x: x.lower()+\"!\")\n",
    "print(s2)\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From dictionaries to series\n",
    "\n",
    "[Video.](https://ceu.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=8a39629f-df2c-4b20-85b1-acbd010a3cbc)\n",
    "\n",
    "So far we converted lists to create series, we can also use a python dictionaries.\n",
    "\n",
    "Let's say that I have my ratings of my favorite actors in a dictionary and let's convert this into a series: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's create a dict of the average movie ratings from 1 to 10 of actors.\n",
    "actor_rating_dict = {'Nicolas Cage':3,'Robert Redford':5,'Julianne Moore':8,\n",
    "                     'Jeff Bridges':7, 'Idris Elba':8,'Meryl Streep':9,\n",
    "                     'Pam Grier':9, 'Dorottya Udvaros':7.5}\n",
    "actor_rating_series = pd.Series(actor_rating_dict)\n",
    "print(actor_rating_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the keys of the dictionary are mapped to index labels.\n",
    "\n",
    "As we have seen before, we can now look up values based on index label or by position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#look up by index label\n",
    "print(actor_rating_series['Idris Elba'])\n",
    "#look up by index position\n",
    "print(actor_rating_series[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look, I have another dictionary! This dictionary stores the number of movies I have seen with the actors in them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating another series: this time how many movies an actor has played in.\n",
    "actor_frequency_dict = {'Nicolas Cage':20,'Robert Redford':6,\n",
    "                        'Julianne Moore':10, 'Jeff Bridges':2,\n",
    "                        'Idris Elba':14,'Mr. Bean':3,'Meryl Streep':7,\n",
    "                        'Pam Grier':11,'Dorottya Udvaros':5}\n",
    "\n",
    "actor_frequency_series = pd.Series(actor_frequency_dict)\n",
    "actor_frequency_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can combine any number of series with the concat command,  what is returned is a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.concat([actor_rating_series, actor_frequency_series], axis=1, sort=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does Mr. Bean's `NaN` mean? I have seen three movies with Mr. Bean, but for some reason I didn't rate him. If the `concat()` function encounters a key that is missing from one of the dictionaries, it substitutes the missing value with the special value `NaN`, which stands for not-a-number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tiny exercise\n",
    "What happens if we exclude `axis=1` from the concat command? Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can rename the columns to have more descriptive labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.columns = ['Average_Rating','Number_of_Movies']\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access columns by name, this returns a series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Number_of_Movies']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or this is the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Number_of_Movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access elements by index label using `loc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#element: [row_label,column_label]\n",
    "print(df.loc['Mr. Bean','Number_of_Movies'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or we can access entire rows and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#row: [row_name]\n",
    "print(df.loc['Mr. Bean'])\n",
    "print(type(df.loc['Mr. Bean']))\n",
    "print('-------------')\n",
    "\n",
    "#column: [:,column_name]\n",
    "print(df.loc[:,'Number_of_Movies'])\n",
    "print(type(df.loc[:,'Number_of_Movies']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows and colunms are returned as series objects.\n",
    "\n",
    "Or we can do the same thing by position using `iloc`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#element\n",
    "print(df.iloc[5,1])\n",
    "print('-------------')\n",
    "\n",
    "#row\n",
    "print(df.iloc[5])\n",
    "print('-------------')\n",
    "\n",
    "#column\n",
    "print(df.iloc[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also do slicing a la NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0:4,0:1] #the first slice is rows, the second is columns, just like in NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns a dataframe.\n",
    "\n",
    "You can also do masking. The most common way to use this, is to filter rows by creating a mask that has as many `True` or `False` values as the number of row. For example, to get all actors with rating larger than 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Average_Rating']>6.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with missing values\n",
    "\n",
    "[Video](https://ceu.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=58cfe757-1aed-4d44-ba11-ab8a00dc9c7e)\n",
    "\n",
    "A common task in data processing is to deal with missing data. For example, we don't have a rating for Mr. Bean. One possibility is that we make an educated guess, and say that Mr. Bean has the same rating as the average of everyone else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Average_Rating'] returns a series corresponding to the column\n",
    "#and series has a method to calculate its mean\n",
    "avg = df['Average_Rating'].mean() \n",
    "print('Average of Average Rating = %g' % (avg))\n",
    "\n",
    "#We can change the elements directly:\n",
    "df.loc['Mr. Bean','Average_Rating'] = avg\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is such a common task, that pandas has a built in method to locate and replace `NaN` called `.fillna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set Mr. Bean's average rating back to np.nan\n",
    "df.loc['Mr. Bean','Average_Rating'] = np.nan\n",
    "\n",
    "# override the Average_Rating column with a version that has the nan's replaced by the average\n",
    "# of the non-nan entries.\n",
    "df['Average_Rating'] = df['Average_Rating'].fillna(np.mean(df['Average_Rating']))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using apply() with a dataframe\n",
    "\n",
    "[Video](https://ceu.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=dd8d9677-fa4a-4493-9047-ab8a00e03354)\n",
    "\n",
    "We seen before that you can get a column as a series, this way you use `apply()` like we did before. For example, we can create a series indicating our favorite actros: it will contain 1 if the rating of the actor is higher than 7 otherwise 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "favorites = df['Average_Rating'].apply(lambda x: 1 if x>7. else 0)\n",
    "print(favorites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can use this to create a new column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['favorites']=favorites\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we might want to use more than one column as input. For example, I would like to create a new column for that will indicate actors that I don't like, yet I've seen many times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a fuction that we will use with apply\n",
    "def func(row):\n",
    "    if row['Average_Rating']<=5 and row['Number_of_Movies']>=10:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "love_to_hate = df.apply(func, axis=1)\n",
    "df['love_to_hate'] = love_to_hate\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The setting `axis=1` tells `apply()` to iterate through rows, `axis=0` iterates through columns. Take look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.apply(np.mean, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Create a new column `love_to_love` that contains a `1` for actors that have rating at least `7` and I have seen movies with them at least `10` times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "#define a fuction that we will use with apply\n",
    "def func(row):\n",
    "    if row['Average_Rating']>=7 and row['Number_of_Movies']>=10:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "love_to_love= df.apply(func, axis=1)\n",
    "df['love_to_love'] = love_to_love\n",
    "df\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting\n",
    "\n",
    "Pandas has built in plotting to quickly take care of common plots. It sits 'on top of' matplotlib and so can be customized in the same way.\n",
    "\n",
    "The pandas `plot()` function returns the matplotlib `Axes` object of the figure that it created. You can use this `Axes` to customize your figure.\n",
    "\n",
    "Creating an automatically labelled bar chart is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax = df.plot(kind='bar', use_index=True, y='Average_Rating')\n",
    "ax.set_title('Actor Number of Films vs Avg. Rating', size=15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or a histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax=df.plot(kind='hist', y='Number_of_Movies', legend=True)\n",
    "\n",
    "ax.set_title('Number of Movies Histogram');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we know the basics, let's do something more complex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II: Surviving the Titanic\n",
    "\n",
    "[Video](https://ceu.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=41bee325-1d33-4211-a829-ab8a00e2e714)\n",
    "\n",
    "Pandas has great data manipulation abilities. Let's finally consider some real data first. First we are going to consider passenger data from the Titanic, which sunk on its maiden voyage. Of 2,224 passengers and crew, more than 1,500 died."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a data file containing data of some of the passengers, this is how it looks like (this will work on Linux or Mac, but not on Windows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!head titanic.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from a csv in pandas is very easy! `read_csv` is very flexible: can take `txt`, plain files, and many more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('titanic.csv', header=0, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`header = 0` indicates that the first row is the header. In this case it is not necessary. `sep` is the column seperator, other common examples include tabs `\\t`, white space, and `|`.\n",
    "\n",
    "Note that pandas automatically guesses the datatype of each column and converts it appropriately. This usually works, but in some unusual cases it might fail, for example, phone numbers might be converted to numbers instead of kept as strings. In these cases the `dtype` argument can be used to specify the data type by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does are dataframe look like? We have too many rows to print out the whole table, but we can use the `head()` method to show only the first five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tail method reads the last five rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some more information about the data:\n",
    "- Pclass: passenger class. \n",
    "- SibSp: number of siblings+spouses aboard\n",
    "- Parch: number of parents+children aboard\n",
    "- Fare: cost of ticket\n",
    "- Cabin: room ID, if passenger had a room\n",
    "- Embarked: port of departure (C= Cherbourg; Q= Queenstown; S=Southampton)\n",
    "\n",
    "### Let's check out a few data exploration techniques\n",
    "\n",
    "Basic statistics are printed out by the `describe()` method for numeric columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize pairs of variables quickly. The `pd.plotting.scatter_matrix()` function creates a matrix of plots: the diagonals contain histograms, and the off-diagonal plots are scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(df[['Parch','Age','Fare']],figsize=(8,8));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where `df[['Parch','Age','Fare']]` selects three columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Grouping\n",
    "\n",
    "[Video](https://ceu.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=c74c32a0-0766-4867-90e7-ab8a00e52015)\n",
    "\n",
    "Pandas has powerful grouping methods that allows us to group entries based on a column value.\n",
    "\n",
    "For example, we can ask the question does the average survival rate depend on the passengers ticket class? For this we group passengers based on the colunm `Pclass`, this creates three groups. Then we calculate the average survival rate for each group separetly by calculating the mean of the `Survived` column. (Remeber this column is 1 if the passenger survived, 0 if they died; therefore the mean is the survival rate!)\n",
    "\n",
    "These steps are done easily with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Pclass')['Survived'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "What about \"women and children first\"? Does sex correlates with survival rate? Calculate the average survival rate for men and women separately!\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "Do the same thing as in the previous example, only this time group by `Sex` instead of `Pclass`.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "df.groupby(['Sex'])['Survived'].mean()\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do even more refined grouping. Let's combine the two: groupby both class and sex, and calculate the survival rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_by_class_and_sex = df.groupby(['Pclass','Sex'])['Survived'].mean()\n",
    "survived_by_class_and_sex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these survival rates!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "survived_by_class_and_sex.unstack(1).plot(kind='bar', title='Survival Probability by Sex and Class');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "In the previous example we used the `unstack()` method. What does this do to grouped data? Try `unstack(0)` and `unstack(1)` in the next cell, also try the plot without using `unstack()`. Explain what the the function does!\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "In addition to trying out plots, print out `survived_by_class_and_sex`, `survived_by_class_and_sex.unstack(0)`, and `survived_by_class_and_sex.unstack(1)`. What is their type? What are the indices?\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "The call `df.groupby(['Pclass','Sex'])['Survived'].mean()` returns a series object with [hierarchical indices](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html), where `Pclass` is the level 0 index and `Sex` is the subindex at level 1. The method `unstack()` transforms the 1 dimensional series with hierarchical indices to a 2 dimensional data frame where the column names and row names are the two levels of indices.\n",
    "\n",
    "Try these lines in separate cells:\n",
    "\n",
    "```python\n",
    "survived_by_class_and_sex\n",
    "survived_by_class_and_sex.unstack(1).plot(kind='bar', title='Survival Probability by Sex and Class')\n",
    "survived_by_class_and_sex.unstack(1)\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsetting\n",
    "\n",
    "As we mentioned before, filtering the data based on some condition is also simple. If you remember, this is similar to using boolean masks in NumPy.\n",
    "\n",
    "We can subset the data to only include passengers below 30:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "under_30=df[df['Age']<30]\n",
    "under_30.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Subset the data to only include passengers that payed less than average fare.\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "Calculate the mean fare using `df['Fare'].mean()`.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "avg_fare = df['Fare'].mean()\n",
    "print(\"The average fare is\", round(avg_fare,3))\n",
    "below_average_fare = df[df['Fare']<avg_fare]\n",
    "below_average_fare.head()\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do more complicated filtering using logical operators. (Remember `&`=and and `|`=or)\n",
    "\n",
    "For example, to select the male passengers who got on the Titanic in France (Cherbourg is in France) we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "males_from_france = df[(df['Embarked']=='C') & (df['Sex']=='male')]\n",
    "males_from_france.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New columns\n",
    "\n",
    "We can also create new columns! Let's count the reverends on board.\n",
    "\n",
    "First we define a helper function that takes a name as input and returns 1 if they are a reverend and 0 if they are a layman. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_rev(input_name):\n",
    "    # they are a reverend if their name contains the 'Rev.' title\n",
    "    if 'Rev.' in input_name:\n",
    "        return 1  \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test the function, we can apply it elementwise to the `Name` column and count the number of reverends on board:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df['Name'].apply(is_rev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define a new column we can simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['is_reverend'] = df['Name'].apply(is_rev)\n",
    "\n",
    "#check the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `apply()` for a function that that takes multiple columns as input. For example, we can count the number of revereneds that are older than 50: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(df.apply(lambda row: 1 if 'Rev.' in row['Name'] and row['Age']>50 else 0, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the attribute `axis=1`, this tells `apply()` to apply the function to each row, `axis=0` would apply it to each column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise -- New column\n",
    "\n",
    "Add a new column, fancy_title, by writing a function that checks the passenger name for a fancy title like \"Master\" or \"Colonel\" or \"Count\".\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "Look at the example where we added a new column to indicate reverends, we have to do the same here only this time you have to check if any of the possible titles appear in the `Name` column.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "fancy_title = df['Name'].apply(lambda x: 1 if \"Master\" in x or \"Colonel\" in x or \"Count\" in x else 0)\n",
    "df['fancy_title'] = fancy_title\n",
    "\n",
    "family_on_board = df.apply(lambda x: 1 if x['SibSp']>0 or x['Parch']>0 else 0, axis=1)\n",
    "df['family_on_board']=family_on_board\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III: Bikesharing with Pandas\n",
    "\n",
    "[Video](https://ceu.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=fd4f3a1a-5b98-4a17-be01-ab8b00d637a2)\n",
    "\n",
    "Let's look at bike sharing data from Chicago. In Chicago there is a bike-sharing program just like Budapest's Bubi, called Divvy. We have two datasets (retrieved from https://www.divvybikes.com/system-data): one on trips and another on stations. We'll have to combine the two to explore how people use Divvy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trips = pd.read_csv('Divvy_Trips_2013.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Side note: Did you get a warning message? You can safely ignore it, everything will work in the notebook. You got it because one of the columns has a mixed datatype (string and NaN), which can be detrimental to the performance of pandas. To get rid of it, you can force the column to be string only by setting `dtype={'gender':'string'}` in the argument of the `read_csv` function.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many trips does our data set contain? In other words, how many rows are there in the dataframe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trips)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic summary of data frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Column meanings:\n",
    "- trip_id: trip identifier\n",
    "- starttime: time bike was rented\n",
    "- stoptime: time bike was returned\n",
    "- bikeid: bike identifier\n",
    "- trip duration: duration of the trip in seconds\n",
    "- from_station_id: station id from which bike was borrowed\n",
    "- from_station_name: station name from which bike was borrowed\n",
    "- to_station_id: station id to which bike was returned\n",
    "- to_station_name: sstation id to which bike was returned\n",
    "- usertype: Customer or subscribers. Customers are one time users that pay for single trips, subscribers buy a monthly -pass with unlimited rides.\n",
    "- gender: Gender, if known, of user\n",
    "- birthyear: year of birth, if known, of user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to continue with exploring the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heterogeneous trip durations\n",
    "\n",
    "[Video](https://ceu.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=734d14b0-992c-41e9-bc16-ab8b00d76ffc)\n",
    "\n",
    "For example, let's plot the trip duration distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips['tripduration'].plot(kind='hist',bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not too helpful, we have a few trips that are much longer than average. \n",
    "Let's create a new column with the logarithm of the tripduration and plot that.\n",
    "\n",
    "We can use the `apply()`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips['logduration']=trips['tripduration'].apply(np.log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or since `np.log()` is a numpy function, we can simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips['logduration']=np.log(trips['tripduration'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax=trips['logduration'].plot(kind='hist',bins=50,color='#9BCC31')\n",
    "ax.set_xlabel('Log(trip duration)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks better. What this means is that tripduration has a very wide distribution, which is closer to a lognormal than a normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most used bikes\n",
    "\n",
    "Let's see which bike is on the road the longest: perhaps a good candidate for early retirement. The `bikeid` column uniquely identifies the bikes, to find all trips associated to each bike we can use the `groupby()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_usage = trips.groupby('bikeid')['tripduration'].sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack this line of code:\n",
    "1. `groupby(bikeid)`: this creates a group for each individual bike\n",
    "2. `['tripduration']`: pick the `tripduration` column\n",
    "3. `sum()`: sum all trips for each bike, so we get a series containing the total usage and indexed by the bike IDs\n",
    "4. `sort_values(ascending=False)`: sort in descending order so that the most used bike is in the first row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most used bikes are in the first rows, we can print out the top 10 using `head()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bike_usage.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least used bikes are at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bike_usage.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = bike_usage.hist(bins=40,color='g');\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.set_xlabel(\"total usage\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Repeat the previous analysis, but instead of the total duration of usage, look at the total number of times the bikes were used. Do you get the same bikes in the top 10?\n",
    "\n",
    "Previously we used the `sum()` function to aggregate the data for each bike, now you can use the `count()` function, which simply counts the number of elements in the group.\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "    \n",
    "The only difference compared to the previous example is the `sum` versus `count`.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "bike_usage2 = trips.groupby('bikeid')['tripduration'].count().sort_values(ascending=False)\n",
    "ax = bike_usage2.hist(bins=40,color='g');\n",
    "ax.set_ylabel(\"count\")\n",
    "ax.set_xlabel(\"total number of times used\");\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timestamps\n",
    "\n",
    "[Video](https://ceu.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=bc18a9de-d075-4d9f-9f7c-ab8b00d9ee84)\n",
    "\n",
    "We can use the timestamps associated to each trip to investigate such questions as how usage depends on the time of the year, day of the week or part of the day.\n",
    "\n",
    "How can we make use of these timestamps? First, let's see how the timestamps are currently stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(trips['starttime'][0])\n",
    "print(type(trips['starttime'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn those time columns into pandas' timestamp objects, which are similar to python's datetime objects. It is a common task so pandas has a built in function for it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips.starttime = pd.to_datetime(trips.starttime, format=\"%Y-%m-%d %H:%M\")\n",
    "trips.stoptime = pd.to_datetime(trips.stoptime, format=\"%Y-%m-%d %H:%M\")\n",
    "\n",
    "print(trips['starttime'][0])\n",
    "print(type(trips['starttime'][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Side note: we can access a column by name two ways, `trips['starttime']` is the same as `trips.starttime`.)\n",
    "\n",
    "Note about `to_datetime()`: if you don't pass a format pandas will try to guess it. It does a decent job, but takes slower and is risky. One can also pass `errors = \"coerce\"` and it will return `NaT` (Not a Time) values for those entries that don't fit into the format you give."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these columns to plot stuff. For example, let's see the variation in duration by start time across the year. Plotting as a function of time has never been easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ax = trips.plot(kind='line', x='starttime', y='tripduration', figsize=(14,5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a mess, let's try to instead look at daily averages. Currently the timestamps have minute precision, let's create a column that only contains the day. For this, we can use the `date` method of timestamp objects. As a demonstration let's apply it to the first timestamp in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips['starttime'][0].date()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a new column using `apply()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips['startdate']=trips['starttime'].apply(lambda dt: dt.date())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we group by the date and calculate the average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_avg_tripduration = trips.groupby('startdate')['tripduration'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This now a series that is indexed by the date and contains the average trip duration for each day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_avg_tripduration.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = daily_avg_tripduration.plot(kind='line',figsize=(14,5));\n",
    "ax.set_ylabel('Daily avg. trip duration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see two patterns:\n",
    "* There is an overall decreasing trend: people go on shorter trips in the winter\n",
    "* Usage is periodic\n",
    "* Bonus question: can you guess why might we see that peak on the first day? (Hint: think of the shape of the distribution of the trip duration and sample sizes.)\n",
    "\n",
    "Let's investigate the origin of the periodicity! Perhaps it is a weekly pattern? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Repeat the previous analysis, but instead of doing a daily average for each day, calculate the average for the days of the week, i.e., you will get seven datapoints, the average trip duration for each day of the week.\n",
    "* Create a new column named `'dayofweek'` that contains the day of the week the trip started\n",
    "* Group the trips based on this new column\n",
    "* Plot the result\n",
    "\n",
    "What did you find?\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "To get the day of the week using the `dayofweek` attribute of timestamps. Pay attention: this is not a function, there is no parenthesis at the end. For example, the day of week of the first timestamp is `trips.['starttime'][0].weekofday`.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "trips['dayofweek']=trips['starttime'].apply(lambda dt: dt.dayofweek)\n",
    "dayofweek_avg_tripduration = trips.groupby('dayofweek')['tripduration'].mean()\n",
    "ax = dayofweek_avg_tripduration.plot(kind='line',figsize=(10,5));\n",
    "ax.set_ylabel('average trip duration');\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pandas `merge`: adding station data\n",
    "\n",
    "Recall that we also have information about stations in 'Divvy_Stations_2013.csv'. Let's have a look at this data and add it to our trips dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations = pd.read_csv('Divvy_Stations_2013.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(stations))\n",
    "stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we have a much smaller dataframe with totally different rows. We want to join the stations dataframe to the trips dataframe so that we have lat/long/capacity data for each trips' starting and ending stations. The name column in the stations frame corresponds to the `from_station_name` and `to_station_name columns` in the trip frame. For this task we can use the [merge()](https://pandas.pydata.org/pandas-docs/version/0.23.4/generated/pandas.merge.html) function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Video](https://ceu.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=b3628637-bd80-47f6-8225-ab8b00dd9d62)\n",
    "\n",
    "First, merge trips and stations for the starting stations. Let's unpack the following function call:\n",
    "* `merge()` takes two dataframes `left` and `right`\n",
    "* `how='left'` means that we take the `left` dataframe, in this case `trips`, and we add new columns to it based on the `right` dataframe, in this case `stations`. Therefore in the new dataframe we will have the same number of rows as in `trips`.\n",
    "* `left_on='from_station_name'` and `right_on='name'`: `merge()` takes a rew in `left` looks at the `from_station_name` and looks for a match in the `right` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trips2 = pd.merge(left=trips, right=stations, how='left', left_on='from_station_name', right_on='name')\n",
    "\n",
    "trips2[['trip_id',\"starttime\",\"from_station_name\",\"to_station_name\",'latitude','longitude']].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a dataframe that contains the trips + information about the starting station. For example, the `lattitude` column contains the lattitude of the starting station.\n",
    "\n",
    "We also want to add the information about the destination stations. If we do the same as before, we would get duplicate column names, e.g., two `lattitude` columns. To avoid this `merge()` can add a suffix to the column names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#added a suffix list for duplicated names\n",
    "trips_extended = pd.merge(trips2, stations, how='inner', left_on='to_station_name', right_on='name',\n",
    "                    suffixes=['_origin', '_dest'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the first three rows of the new dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trips_extended.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a bunch of new data to work with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "What are the most popular routes? Create a dataframe called `station_to_station` that has a row for each station origin-destination pair and has columns:\n",
    "* `from_station_name`\n",
    "* `to_station_name`\n",
    "* And a column containing the total number of trips between the two stations. (Bonus: rename this column `numtrips`.)\n",
    "* Print out the top 10 station pairs\n",
    "\n",
    "<details><summary><u>Hint 1.</u></summary>\n",
    "<p>\n",
    "\n",
    "Group `trips_extended` by two columns: `'from_station_name'` and `'to_station_name'`.\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "<details><summary><u>Hint 2.</u></summary>\n",
    "<p>\n",
    "\n",
    "To aggregate the data for each group use `count()`.\n",
    "\n",
    "</p>\n",
    "</details>\n",
    "\n",
    "\n",
    "<details><summary><u>Hint 3.</u></summary>\n",
    "<p>\n",
    "\n",
    "You can rename columns using `df.rename(columns = {'old_col_name':'new_col_name'},inplace=True)`. For details look up the documentation!\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "station_to_station = trips_extended.groupby(['from_station_name','to_station_name'])['trip_id'].count().reset_index()\n",
    "station_to_station.rename(columns = {'trip_id':'numtrips'},inplace=True)\n",
    "station_to_station.sort_values('numtrips', ascending=False).head()\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Plotting with Seaborn\n",
    "[Video.](https://ceu.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=2896342a-c826-4fef-8fa6-acc200ce5455)\n",
    "\n",
    "Pandas plotting is simple but limited, matplotlib plotting is complicated but powerful. A library called [seaborn](https://seaborn.pydata.org/) is in the middle and it works very well with data in pandas. We will look at few things that we can do with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('ticks')# you can try other styles: white, dark..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Why is seaborn imported as `sns`? [The West Wing.](https://en.wikipedia.org/wiki/Sam_Seaborn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do women and men use Divvy the same way? Let's take the figure that we made in the previous exercise, but separate the average trip duration for the two groups. We can use `groupby()` to group according to two columns by passing a list containing the names of the column to `groupby()` (we did something similar with the Titanic dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips['dayofweek']=trips['starttime'].apply(lambda dt: dt.dayofweek) # in case you have skipped the previous exercise\n",
    "dayofweek_gender_duration = trips.groupby(['dayofweek','gender'])['tripduration'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a series with hierarchical indices (`dayofweek` and `gender`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dayofweek_gender_duration.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For plotting we will need a dataframe. We can use `reset_index()` to convert the indices to regular columns and index the rows with integers instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dayofweek_gender_duration = dayofweek_gender_duration.reset_index()\n",
    "dayofweek_gender_duration.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is now a dataframe which is indexed by integers, and the previous index labels became regular columns `dayofweek` and `gender`.\n",
    "\n",
    "We use seaborn to create a fancy plot! Let's use the `pointplot` function. If you pass the name of a column that contains a categorical value (e.g., `gender`) as the `hue` argument, the `pointplot` function will plot them as separate lines with different colors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ax=sns.pointplot(data=dayofweek_gender_duration,x='dayofweek',y='tripduration', hue='gender')\n",
    "ax.set_ylabel('Average duration of trips [s]')\n",
    "ax.set_title('Average trip duration per day user gender');\n",
    "# Seaborn has a bunch of formatting options\n",
    "# e.g., despine removes top and right sides of the frame\n",
    "#sns.despine()\n",
    "#sns.despine(trim=True,offset=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to pandas' uses matplotlib to create the plots. And similar to pandas' `plot` function, seaborn's `pointplot` also returns the current `Axes`. You can use this `Axes` to further customize your plot directly using matplotlib.\n",
    "\n",
    "So what do we see on the figure? It seems that men go on shorter duration trips, but their trip length increases more on weekends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Our dataset also has a column named `usertype`, it differentiates between customer or subscribers. Customers are one time users that pay for single trips, subscribers buy a monthly pass with unlimited rides.\n",
    "\n",
    "Do customers and subscribers behave differently? Repeat the above analysis for these categories. In addition to trip duration, compare total number of trips as well in a separate cell. What pattern do you see?\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "You can do exatly the same as before only:\n",
    "* group by `usertype` instead of `gender`\n",
    "* also repeat your plot for `count()` instead of `mean()`\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "\n",
    "dayofweek_usertype_duration = trips.groupby(['dayofweek','usertype'])['tripduration'].count().reset_index()\n",
    "ax=sns.pointplot(data=dayofweek_usertype_duration,x='dayofweek',y='tripduration', hue='usertype')\n",
    "ax.set_ylabel('Number of trips')\n",
    "ax.set_title('Number of trips per user type');\n",
    "sns.despine()\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have to restrict ourselves to looking at the only at the daily average. Seaborn has a convinient way to plot distributions for different categories using `FacetGrid`. It creates a grid of plots where the rows and columns can represent a catergory each. Let's plot the logarithm of the tripduration for each day and the two usertypes.\n",
    "\n",
    "(To show the histograms, we are going to use the [kernel density estimate](https://en.wikipedia.org/wiki/Kernel_density_estimation) plot of seaborn, which is basically a smoothed version of the raw histogram.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(trips, col='dayofweek', row='usertype') #specify the grid\n",
    "g.map(sns.kdeplot, 'logduration') #specify the type of plot, here: kernel density estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the color of the lines to represent a category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(trips, col='dayofweek', hue='usertype', col_wrap=4) #col_wrap\n",
    "g.map(sns.kdeplot, 'logduration') #specify the type of plot, here: kernel density estimate\n",
    "g.add_legend(); #add legend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, using rows, columns, and hue we can represent three types of categories simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Video.](https://ceu.cloud.panopto.eu/Panopto/Pages/Viewer.aspx?id=bb03d79f-e724-4060-9b54-acc200d33de6)\n",
    "\n",
    "This figure looks okay, but we can make this better. Uncomment the lines one-by-one in the next cell to see how we format the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#sns.set_style(\"white\", {\"axes.facecolor\": (1.,1.,1.,0.)}) # changes the face to transparent globally\n",
    "#not only this figure\n",
    "pal = sns.cubehelix_palette(7, start=2.8, rot=.75, light=.7) # define a new color palette\n",
    "g = sns.FacetGrid(trips, row='dayofweek',col='usertype',\n",
    "                  #hue='dayofweek', # color the rows differently\n",
    "                  #aspect=6, #aspect ratio of subplots\n",
    "                  #height=1., #height of subplots \n",
    "                  #margin_titles=True, #indicate categories at the top and end of columns and rows\n",
    "                  #palette=pal #use new palette\n",
    "                 )\n",
    "g.map(sns.kdeplot, 'logduration',\n",
    "      #shade=True, #color area under the curve\n",
    "      #alpha=.5,   # make color semitransparent\n",
    "      linewidth=1.5 \n",
    "     ) \n",
    "\n",
    "#g.set_titles(row_template = '                        ', col_template = '{col_name}') #change how categories are printed\n",
    "# the spaces are needed because of a bug in the windows version of seaborn \n",
    "#g.add_legend(); #add legend to indicate the days instead of row labels\n",
    "\n",
    "#g.set(xlim=(4,10)) # set the range of x displayed to focus on the bulk of the distribution\n",
    "#g.set(xticks=[np.log(100),np.log(1000),np.log(10000)]) #change location of ticks\n",
    "#g.set(xticklabels=['log(100)','log(1000)','log(10000)']) #and labels to be more informative\n",
    "\n",
    "\n",
    "#g.fig.subplots_adjust(hspace=-.5) #make subplots overlap\n",
    "\n",
    "# remove axes details that don't play well with overlap\n",
    "#g.set(yticks=[]) # remove y axis and its trappings\n",
    "#the y axis is shared so we can still compare the shape of the distributions\n",
    "#g.despine(bottom=True, left=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am not sure if this looks good or just unusual, but at least we demonstrated some formatting options. Check out the [gallery of examples](https://seaborn.pydata.org/examples/index.html) and the [tutorials](https://seaborn.pydata.org/tutorial.html) to get inspired."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To color the plots above we used a [cubehelix palette](https://seaborn.pydata.org/generated/seaborn.cubehelix_palette.html). This is great palette to use for heat maps too, because the intensity of the color monotonically changes through the palette, meaning that it prints well on black-and-white printers, and even people with any type of colorblindness are able to read it. For more details check out this [blog post](https://ifweassume.blogspot.com/2013/05/cubehelix-or-how-i-learned-to-love.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance between stations\n",
    "\n",
    "Our dataset does not contain information about the trip length, but we do have the latitude and longitude coordinates of the stations. This allows us to calculate the distance between the origin and destination stations, but note that this is not the distance you have to travel on the road, but distance as the crow flies.\n",
    "\n",
    "To do this we have to recall an old favorite: the [haversine formula](https://en.wikipedia.org/wiki/Haversine_formula) which takes a pair of latitude-longitude coordinates and returns their distance in kilometeres:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def latlongdist(lat1,long1,lat2,long2):\n",
    "    rlat1 = math.radians(lat1)\n",
    "    rlat2 = math.radians(lat2)\n",
    "    rlong1 = math.radians(long1)\n",
    "    rlong2 = math.radians(long2)\n",
    "    dlat = rlat2 - rlat1\n",
    "    dlong = rlong2 - rlong1\n",
    "    a = math.sin(dlat / 2)**2 + math.cos(rlat1) * math.cos(rlat2) * math.sin(dlong / 2)**2\n",
    "    c = 2 * math.atan2(math.sqrt(a), math.sqrt(1 - a))\n",
    "    return 6371.0 * c\n",
    "\n",
    "print(latlongdist(48.105625, 20.790556, 46.07308, 18.22857))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new column for the `trips_extended` dataframe that contains the distance using the `apply()` method of the dataframe. Remember in this case the function that we apply has to take the entire row as an input (see previous class, if you don't remember). However, `latlongdist(lat1,long1,lat2,long2)` takes four floats as an input and not the row. How can we solve this? With a lambda function of course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_extended['dist']=trips_extended.apply(lambda row: \n",
    "                                            latlongdist(row['latitude_origin'],\n",
    "                                                        row['longitude_origin'],\n",
    "                                                        row['latitude_dest'],\n",
    "                                                        row['longitude_dest']),\n",
    "                                           axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above lambda function takes a row of the dataframe as input, selects the coordinates and calles the `latlongdist` function.\n",
    "\n",
    "Reminder: `axis=1` indicates that we apply the function to each row, `axis=0` would mean applying the function to each column.\n",
    "\n",
    "Now we let's investigate the new `dist` column!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Make a histogram of the distances between origin and destination stations using pandas' plotting capability. Try different number of bins. Try to set the y axis to logarithmic scale using `ax.set_yscale('log')`. What kind of patterns do you see?\n",
    "\n",
    "<details><summary><u>Hint.</u></summary>\n",
    "<p>\n",
    "\n",
    "To set the y axis to logarithmic scale you have to grab the current `Axes` object. Luckily it is returned by pandas' plot function, so you can write:\n",
    "```python\n",
    "ax=trips_extended['dist'].plot(...)\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "ax=trips_extended['dist'].plot(kind='hist',bins=200)\n",
    "#ax.set_yscale('log')\n",
    "```\n",
    "Looking at the histogram with enough bins it becomes apparent that there is large a peak at zero. This corresponds to trips where the user returned the bike to the same station where they got it from; therefore the origin and destination station is the same.\n",
    "    \n",
    "If we chang the y axis to logarithmic scale, the tail of the distribution becomes a straight line, indicating that the tail decays as an exponential function. The distance distribution is not as heterogeneous as the trip duration distribution.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is reasonable to assume that there is a correlation between station distance and trip duration: the larger the distance the more time is needed for the trip. We could use pandas' for plotting but let's do something more fancy and use seaborn.\n",
    "\n",
    "What is a good plot to visualize the hypothesised correlation? A scatter plot is probably not a good idea, since there are >700,000 trips which would create too many markers. A two dimensional histogram is a better option.\n",
    "\n",
    "We use the `jointplot` function of seaborn which allows to show both the relation between two columns and their marginal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "g = sns.jointplot(\n",
    "    data=trips_extended,\n",
    "    x=\"logduration\", y=\"dist\",\n",
    "    kind=\"hex\",\n",
    "    xlim=(4,9),\n",
    "    ylim=(0,8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn has several plotting options to visualize the relation of two variables, we chose `hex` which is a two dimensional histogram with hexagonal bins. We also set the range of the plot on the x and y axes using `xlim` and `ylim` to focus on the bulk of the distributions (try commenting out those lines). See further plotting options [here](https://seaborn.pydata.org/generated/seaborn.jointplot.html#seaborn.jointplot).\n",
    "\n",
    "So what do we see on the plot? There is a positive correlation as we expected and it seems to be nonlinear. (Remember, we are comparing the logarithm of the trip duration to as-the-crow-flies distance, there is no real reason to expect the correlation to be linear.) An apparent outlier is the zero distance, there are very long trips where the bikes are returned to the origin station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further investigate! If we use `kind=kde` (aka kernel density estimate) instead of `hex`, we can add an additional dimension to the plot: the color of the contours is used to indicate categories. For example, we can compare the occassional customers to subscribers by setting `hue=\"usertype\".\n",
    "\n",
    "(Before you run the next cell: calculating the kde for 700,000 datapoints takes a few minutes. If you don't have that time or you would like to experiment with the plotting options, select a subset of rows for quicker results.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(\n",
    "    data=trips_extended,#[:10000],#apply this slice for quicker plotting\n",
    "    x=\"logduration\", y=\"dist\", hue=\"usertype\",\n",
    "    kind=\"kde\",\n",
    "    xlim=(4,9),\n",
    "    ylim=(0,8),\n",
    "    marginal_kws={'fill':True,'alpha':.5}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the `marginal_kws` to pass on additional formatting settings as a dictionary to the marginal plots: we colored the area under the curves and set them to be transparent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Does the `logduration` distribution depend whether it is a weekend or a weekday? Does it depend on gender? Does it depend if the bike was returned to the same station? Answer these questions doing the following steps:\n",
    "* Create a column indicating if the trip was on a weekend.\n",
    "* Create a column indicating that the bike was returned to the same station.\n",
    "* Create a facet grid using seaborn where rows indicate gender, columns indicate whether the trips starts and ends at the same station, and hue indicates weekend or weekday.\n",
    "\n",
    "<details><summary><u>Hint.</u></summary>\n",
    "<p>\n",
    "\n",
    "You can create the new columns with\n",
    "```python\n",
    "trips_extended['weekend']=trips_extended['dayofweek']>4 #5: Saturday, 6: Sunday\n",
    "trips_extended['samestation']=trips_extended['from_station_id']==trips_extended['to_station_id']\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "trips_extended['weekend']=trips_extended['dayofweek']>4 #5: Saturday, 6: Sunday\n",
    "trips_extended['samestation']=trips_extended['from_station_id']==trips_extended['to_station_id']\n",
    "\n",
    "g = sns.FacetGrid(trips_extended, row='gender',hue='weekend',col='samestation',\n",
    "                  sharex=True,\n",
    "                  sharey=True,\n",
    "                  aspect=2, \n",
    "                  height=2., \n",
    "                  margin_titles=True,\n",
    "                 ) #specify the grid\n",
    "g.map(sns.kdeplot, 'logduration', linewidth=1.5) #specify the type of plot, here: kernel density estimate\n",
    "\n",
    "g.set(xlim=(3,10))\n",
    "g.set(xticks=[np.log(10),np.log(100),np.log(1000),np.log(10000)])\n",
    "g.set(xticklabels=['log(10)','log(100)','log(1000)','log(10000)'])\n",
    "g.add_legend(); #add legend\n",
    "```\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
