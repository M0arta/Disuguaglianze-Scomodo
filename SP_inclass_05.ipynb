{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scientific Python\n",
    "## Central European University\n",
    "\n",
    "## 05 Error handling, JSON, XML, web scraping -- exercises\n",
    "\n",
    "Instructor: Márton Pósfai, TA: --\n",
    "\n",
    "Email: posfaim@ceu.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Don't forget:* use the Slack channel for discussion, to ask questions, or to show solutions to exercises that are different from the ones provided in the notebook. [Slack channel](http://www.personal.ceu.edu/staff/Marton_Posfai/slack_forward.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises -- Error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01 Dictionaries and errors\n",
    "\n",
    "Write a function that does the same thing as `get` method of dictionaries:\n",
    "* takes a dictionary and a key as input\n",
    "* if the key exists, return the corresponding value\n",
    "* if the key does not exist, return `None`\n",
    "\n",
    "Use the `try`-`except` pair!\n",
    "\n",
    "<details><summary><u>Hint.</u></summary>\n",
    "<p>\n",
    "\n",
    "If you try to access a key that doesn't exist, python throws a `KeyError`.\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = {'apple':100, 'watermelon':200,'orange':14}\n",
    "\n",
    "#behavior of the get method:\n",
    "print(D.get('apple'))\n",
    "print(D.get('sausage'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "D = {'apple':100, 'watermelon':200,'orange':14}\n",
    "def same_as_get(d,key):\n",
    "    try:\n",
    "        return d[key]\n",
    "    except KeyError:\n",
    "        return None\n",
    "    \n",
    "print(same_as_get(D,'apple'))\n",
    "print(same_as_get(D,'sausage'))  \n",
    "```\n",
    "\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02 Conversion\n",
    "\n",
    "Write a function that takes a string as input, if possible, converts it to an integer using the `int()` function and returns this value, if the conversion is not possible, it returns `None`.\n",
    "\n",
    "\n",
    "<details><summary><u>Hint.</u></summary>\n",
    "<p>\n",
    "\n",
    "Run `int(\"hello\")` and see what error is thrown. Or you can try to find the right error code [here](https://docs.python.org/3/tutorial/errors.html).\n",
    "\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "def IntConv(x):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except ValueError:\n",
    "        return None\n",
    "    \n",
    "IntConv(\"2.\") \n",
    "```\n",
    "\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 Indexing\n",
    "\n",
    "Write a function that takes one variable `x` as input and returns an element of it indexed by 2 (i.e., `x[2]`) if possible. If not possible return `None` and print out:\n",
    "* \"Index is out of range, buddy.\" if the index is out of range,\n",
    "* \"Please pay more attention to your variable types.\" if `x` is not indexable,\n",
    "* \"Something is not working...\" in any other case.\n",
    "\n",
    "Note that you can handle multiple error types within the same `try` statement simply by adding multiple `except` statements:\n",
    "```python\n",
    "try:\n",
    "    some code\n",
    "except Error1:\n",
    "    some code\n",
    "except Error2:\n",
    "    some code\n",
    "```\n",
    "\n",
    "Write cases to test all error messages.\n",
    "\n",
    "<details><summary><u>Hint.</u></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "Try out these `x`s to see the possible error codes:\n",
    "```python\n",
    "x=0\n",
    "x[2]\n",
    "    \n",
    "x=[1,2]\n",
    "x[2]\n",
    "    \n",
    "x={'hello':33}\n",
    "x[2]\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "def index2(x):\n",
    "    try:\n",
    "        return x[2]\n",
    "    except IndexError:\n",
    "        print(\"Index is out of range, buddy.\")\n",
    "    except TypeError:\n",
    "        print(\"Please pay more attention to your variable types.\")\n",
    "    except:\n",
    "        print(\"Something is not working...\")\n",
    "        \n",
    "    \n",
    "index2({1:[3]}) \n",
    "```\n",
    "\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises -- JSON/webapi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03 JSON\n",
    "\n",
    "Convert the string `S` into a JSON object `D` and do the following:\n",
    "* Try pretty printing `D` (printing in a more human readble way) by converting it back to a string using the argument `indent`, check the documentation for details.\n",
    "\n",
    "<details><summary><u>Hint.</u></summary>\n",
    "<p>\n",
    "\n",
    "To convert back to a string use `json.dumps()`.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = '{\"movies\": {\"Repo Man\": {\"actors\": [\"Emilio Estevez\", \"Harry Dean Stanton\", \"Zander Schloss\"], \"imdb_rating\": 6.9, \"year\": 1984, \"director\": \"Alex Cox\"}, \"Human Highway\": {\"actors\": [\"Neil Young\", \"Mark Mothersbaugh\", \"Pegi Young\"], \"imdb_rating\": 6.0, \"year\": 1982, \"director\": \"Neil Young\"}, \"Mighty Ducks\": {\"actors\": [\"Heidi Kling\", \"Emilio Estevez\"], \"imdb_rating\": 6.6, \"year\": 1992, \"director\": \"Stephen Herek\"}}, \"band_membership\": {\"Emilio Estevez\": [], \"Harry Dean Stanton\": [\"Harry Dean Stanton & The Cheap Dates\"], \"Zander Schloss\": [\"Circle Jks\", \"The Weirdos\"], \"Neil Young\": [\"Crazy Horses\", \"Buffalo Springfield\"], \"Mark Mothersbaugh\": [\"DEVO\"], \"Pegi Young\": [\"Pegi Young and the Survivors\"], \"Heidi Kling\": []}}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "D= json.loads(S)\n",
    "print(json.dumps(D,indent=1))  \n",
    "```\n",
    "\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Print out which movies had their directors also act in them.\n",
    "\n",
    "<details><summary><u>Hint.</u></summary>\n",
    "<p>\n",
    "\n",
    "To test if `x` is contained in a list `L`:\n",
    "```python\n",
    "if x in L:\n",
    "    ...\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "for movie,details in D['movies'].items():\n",
    "    if details['director'] in details['actors']:\n",
    "        print(movie)  \n",
    "```\n",
    "\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Print out the title of the movies and the number of actors in the movie who are also musicians.\n",
    "\n",
    "<details><summary><u>Hint.</u></summary>\n",
    "<p>\n",
    "\n",
    "You have to count the number of actors in `D['movies'][movie_title]['actors']` whose band membership list is not empty.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "#solution 1\n",
    "for movie,details in D['movies'].items():\n",
    "    n = 0\n",
    "    for actor in details['actors']:\n",
    "        if len(D['band_membership'][actor])>0:\n",
    "            n += 1\n",
    "    print(movie,n)\n",
    "    \n",
    "#solution 2\n",
    "for movie,details in D['movies'].items():\n",
    "    n = len([actor for actor in details['actors'] if D['band_membership'][actor]])\n",
    "    print(movie,n) \n",
    "```\n",
    "\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few exercises use `openexchangerates.org`, so we need the app id again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app_id = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04 Monthly rates\n",
    "\n",
    "Download the exchange rates for the first day of each month of 2020 and plot your favorite currency.\n",
    "\n",
    "Tipp: When requesting a lot of data, download and plot the data in separate cells, so that you don't download the data multiple times when you are experimenting with your plot.\n",
    "\n",
    "<details><summary><u>Hint.</u></summary>\n",
    "<p>\n",
    "\n",
    "You can use `datetime` to create the list of strings representing the dates. However, you can do it in a more simple way using `\"2020-%02d-01\"%m` where `m` goes from 1 to 12.\n",
    "    \n",
    "</p>\n",
    "</details>\n",
    "\n",
    "Download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "#download\n",
    "dates = [\"2020-%02d-01\"%t for t in range(1,13)]\n",
    "\n",
    "monthly_rates = []\n",
    "currency = 'BTC'\n",
    "for date in dates:\n",
    "    URL = \"http://openexchangerates.org/api/historical/\" + date + \".json?app_id=\"+app_id\n",
    "    result = urllib.request.urlopen(URL)\n",
    "    text = result.read()\n",
    "    data = json.loads(str(text,\"utf-8\"))\n",
    "    monthly_rates.append(data['rates']['BTC'])\n",
    "\n",
    "#plot   \n",
    "plt.plot(dates,monthly_rates,'-o')\n",
    "plt.ylabel('bitcoin vs USD')\n",
    "plt.xticks(rotation=45);\n",
    "```\n",
    "\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05 Check API usage\n",
    "\n",
    "Write code to check how many data requests did you send to `openexchangerates.org`. Check the API documentation [here](https://docs.openexchangerates.org/docs/usage-json).\n",
    "\n",
    "<details><summary><u>Hint.</u></summary>\n",
    "<p>\n",
    "\n",
    "The URL that you have to request is\n",
    "```python\n",
    "\"http://openexchangerates.org/api/usage.json?app_id=xxx\"\n",
    "```\n",
    "where `xxx` is your App ID\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "URL = \"http://openexchangerates.org/api/usage.json?app_id=\"+app_id\n",
    "result = urllib.request.urlopen(URL)\n",
    "text = result.read()\n",
    "usagedata = json.loads(str(text,\"utf-8\"))\n",
    "usagedata['data']['usage']['requests']\n",
    "```\n",
    "\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06 Old exchange rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What year is the oldest exchange rate data from? To answer this question combine using the API with error handling. \n",
    "\n",
    "<details><summary><u>Hint.</u></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "If you request a date that `openexchangerates.org` doesn't have data for it throws an error. Try requesting data for `2020-12-31`, `2019-12-31`, and so on. The first year for which we get an error will be the first year there is no data for.\n",
    "\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "year = 2020\n",
    "while True:\n",
    "    try:\n",
    "        # build a url from pieces:\n",
    "        base_url = \"http://openexchangerates.org/api\"\n",
    "        id_str   = \"app_id=\"+app_id\n",
    "        date_str = str(year)+\"-12-31\"\n",
    "        #stich it together\n",
    "        URL = base_url+\"/historical/\"+date_str+\".json?\"+id_str # this format is specified at the end of the doc page\n",
    "        result = urllib.request.urlopen(URL)\n",
    "    except:\n",
    "        print(f'The oldest exchange rate data is from {year+1}.')\n",
    "        break\n",
    "    year-=1\n",
    "```\n",
    "\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather API\n",
    "\n",
    "I ran out of ideas for exercies involving exchange rates, so let's try something else.\n",
    "\n",
    "Web API services are typically marketed developers, who create apps or other software that rely on the data from the web APIs. Usually there is a free limited subscription that allows developers to test out an API before committing to it. We make use of of these subscriptions for our own eductaional purposes.\n",
    "\n",
    "Register and get an app id (also known as the api key) from [weatherapi.com](https://www.weatherapi.com/). You need an email address to register. On an unrelated note there are a disposable email [services](https://www.google.com/search?&q=temporary%20email) that provide a temporary email that you can use once and then forget about. Sometimes these email addresses are not allowed by webservices, so if the first one does not work you can \n",
    "\n",
    "There is also an [api explorer](https://www.weatherapi.com/api-explorer.aspx) that let's you test and construct request urls on their website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key='...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07 Current weather\n",
    "\n",
    "This api has a wide range of functionality. For example the following request provides info about the current weather in Paris:\n",
    "```python\n",
    "url = \"http://api.weatherapi.com/v1/current.json?key=\"+api_key+\"&q=Paris\"\n",
    "```\n",
    "Download the current weather, convert it to a JSON object and print it out to explore the data it contains.\n",
    "\n",
    "<details><summary><u>Hint.</u></summary>\n",
    "<p>\n",
    "\n",
    "Use the same steps we used for `openexchangerates.org`.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "url=\"http://api.weatherapi.com/v1/current.json?key=\"+api_key+\"&q=Paris\"\n",
    "result = urllib.request.urlopen(url)\n",
    "text = result.read()\n",
    "current_weather = json.loads(str(text,\"utf-8\"))\n",
    "print(json.dumps(current_weather,indent=1))\n",
    "```\n",
    "\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08 Rain --  Discussion exercise\n",
    "\n",
    "Write a function that takes your location as input and prints out \"yes\" if it is raining outside and \"no\" if it is not.\n",
    "\n",
    "Take a look at the [api documentation](https://www.weatherapi.com/docs/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Share your solution on Slack. We will discuss the problem during class together.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09 Forecast\n",
    "\n",
    "Someone I know moved from Davis, California to Budapest for a new job. Did this person make the right decision? Get tomorrows hourly weather forecast for Davis, California and Budapest and plot the predicted temperature as a function of hour of the day.\n",
    "\n",
    "<details><summary><u>Hint.</u></summary>\n",
    "<p>\n",
    "\n",
    "Request the forecast for two days, the second day in the list will be tomorrow's forecast:\n",
    "```python\n",
    "weather['forecast']['forecastday'][1]\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "for loc in ['Davis','Budapest']:\n",
    "    url=\"http://api.weatherapi.com/v1/forecast.json?key=\"+api_key+\"&q=\"+loc+\"&days=2\"\n",
    "    result = urllib.request.urlopen(url)\n",
    "    text = result.read()\n",
    "    weather = json.loads(str(text,\"utf-8\"))\n",
    "    temps = [hw['temp_c'] for hw in weather['forecast']['forecastday'][1]['hour']]\n",
    "\n",
    "    plt.title(weather['forecast']['forecastday'][1]['date'])\n",
    "    plt.plot(temps,'o-',label=loc)\n",
    "    \n",
    "plt.xlabel('hours')\n",
    "plt.ylabel('temperature[C]')\n",
    "plt.legend()\n",
    "```\n",
    "\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10 Clean air\n",
    "\n",
    "Sort the EU capitals based on current air quality as measured by airborne coarse particulate matter (PM10).\n",
    "\n",
    "<details><summary><u>Hint.</u></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "Add `aqi=yes` to the request URL to ensure that the response contains the airquality info.\n",
    "\n",
    "Create a list of tuples `(city_name,pm10)` and sort the list based on the second element of the tuple.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu_caps=[\"Vienna\", \"Brussels\", \"Sofia\", \"Zagreb\", \"Nicosia\", \"Prague\", \"Copenhagen\", \n",
    " \"Tallinn\", \"Helsinki\", \"Paris\", \"Berlin\", \"Athens\", \"Budapest\", \"Dublin\", \n",
    " \"Rome\", \"Riga\", \"Vilnius\", \"Luxembourg\", \"Valletta\", \"Amsterdam\", \"Warsaw\",\n",
    " \"Lisbon\", \"Bucharest\", \"Bratislava\", \"Ljubljana\", \"Madrid\", \"Stockholm\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "\n",
    "\n",
    "```python\n",
    "pm10 =[]\n",
    "for loc in eu_caps:\n",
    "    url=\"http://api.weatherapi.com/v1/current.json?key=\"+api_key+\"&q=\"+loc+\"&aqi=yes\"\n",
    "    result = urllib.request.urlopen(url)\n",
    "    text = result.read()\n",
    "    weather = json.loads(str(text,\"utf-8\"))\n",
    "    pm10.append(weather['current']['air_quality']['pm10'])\n",
    "\n",
    "joint_list = list(zip(eu_caps,pm10))\n",
    "joint_list.sort(key =lambda pair: pair[1])\n",
    "joint_list  \n",
    "```\n",
    "\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises -- XML/HTML\n",
    "\n",
    "As part of the exercises you will have to extract some data from a file or website using BeautifulSoup and typically you will have to process this data (plot something, calculate a statistic, etc), you can do the second part multiple ways using built-in datatypes, numpy or pandas, the choice is yours. Similar to real applications, you might find that the simplest solution uses a function or trick that we did not cover in class, so look at the documentations and use the internet as needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 Quiz XML\n",
    "\n",
    "As an instructor of a course you can export the quizes in moodle as xml files. The file `python_quiz.xml` contains the questions from the quiz you did after the first class.\n",
    "\n",
    "Load the file and parse it with BeautifulSoup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"python_quiz.xml\",\"r\",encoding='utf-8') as f:\n",
    "    soup = BeautifulSoup(f,\"xml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "with open(\"python_quiz.xml\",\"r\",encoding='utf-8') as f:\n",
    "    soup = BeautifulSoup(f,\"xml\")\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The questions are contained in `<question>` tags. Take the first one and figure out how to pretty print it, so you can examine its structure.\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "`tag.another_tag` returns the first `another_tag` inside `tag`, see class notebook. The `prettify()` tag method creates a formatted string.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "print(soup.question.prettify())\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many questions are there and what are the different types of question?\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "Iterate over all `<question>` tags.\n",
    "    \n",
    "One possible way to identify the different types is to collect all first occurances of the `type` attribute in a list. You can also check out the built-in data type `set`.\n",
    " \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "qtypes = []\n",
    "for q in soup.find_all('question'):\n",
    "    if q['type'] not in qtypes:\n",
    "        qtypes.append(q['type'])\n",
    "print(\"Number of questions:\", len(soup.find_all('question')))\n",
    "print(\"Question types:\", qtypes)\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many questions contain the word \"list\" in the question itself (not the answers)?\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "The questions are encased in the `<questiontext>` tags.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "n_lists =0\n",
    "for q in soup.find_all('question'):\n",
    "    if \"list\" in q.questiontext.text:\n",
    "        #print(q.questiontext.text)\n",
    "        n_lists += 1\n",
    "print(n_lists)\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the `\"essay\"` type questions and double the size of response field (this is the box that you type in, the height of the box is given in lines in the `<responsefieldlines>`tag).\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "The text directly contained within `<responsefieldlines>` is `responsefieldlines.string`, you can simply overwrite this using `responsefieldlines.string=...`.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "for q in soup.find_all('question',type=\"essay\"):\n",
    "    #print(q.responsefieldlines.string)\n",
    "    q.responsefieldlines.string=str(2*int(q.responsefieldlines.string))\n",
    "    #print(q.responsefieldlines.string)\n",
    "#print(soup.question.prettify())\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the number multiple choice questions that have more than one correct answer (the answer tags have an attribute called `fraction`, for correct answers the fraction is larger than zero).\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "Write a function `multiple_correct(tag)` that takes a tag as input and returns `True` if\n",
    "* the tag is a `<question>` and\n",
    "* its `type` attribute is equal to `multichoice` and\n",
    "* it contains at least two `<answer>` tags that have a non-zero `fraction` attribute\n",
    "And otherwise returns `False`.\n",
    "    \n",
    "Use the `multiple_correct(tag)` with `find_all`\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "def multiple_correct(tag):\n",
    "    if tag.name==\"question\" and tag['type']=='multichoice':\n",
    "        n = 0\n",
    "        for a in tag.find_all('answer'):\n",
    "            if float(a['fraction'])>0:\n",
    "                n+=1\n",
    "        if n>1:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "print(len(soup.find_all(multiple_correct)))\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `has_attribute(soup,tag,attribute)` that takes a soup object `soup` and two strings `tag` and `attribute` as input. Looks for the first occurance of `tag` and returns `True` if this tag has an attribute called `attribute` and `False` otherwise.\n",
    "\n",
    "For example, using the quiz xml file `has_attribute(soup,\"question\",\"type\")` returns `True`, but `has_attribute(soup,\"question\",\"horseradish\")` returns `False`.\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "The attributes are stored and accessed like a dictionary, e.g., `soup.question[\"type\"]` provides the value of the `\"type\"` attribute. If you try to access a non-existing attribute, it throws a `KeyError`, which you can catch using a `try`-`except` pair. Or you can use the `get()` dictionary method (remember class 3?).\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "def has_attribute(soup,tag,attribute):\n",
    "    if soup.find(tag).get(attribute)!=None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "#or\n",
    "def has_attribute(soup,tag,attribute):\n",
    "    if soup.find(tag).get(attribute):\n",
    "        return True\n",
    "    return False  \n",
    "\n",
    "print(has_attribute(soup,\"question\",\"type\"),has_attribute(soup,\"question\",\"horseradish\"))\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12 arXiv.org\n",
    "\n",
    "[arXiv.org] is an open-access preprint repository containing almost 2,000,000 scietific papers. It originally started for Physics, but since has expanded to include math, computer science and other fields. It has an API that allows you to access the articles and their metadata. Use the URL below to retrive metadata of the first 1000 articles with titles containing the word \"covid\" in XML format. The information about the individual papers are stored in `<entry>` tags, print out the first one to see the available data.\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "Same as in the first exercise.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "url = 'http://export.arxiv.org/api/query?search_query=ti:\"covid\"&max_results=1000&sortBy=submittedDate&sortOrder=ascending'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "data = urllib.request.urlopen(url)\n",
    "soup = BeautifulSoup(data,\"xml\")\n",
    "print(soup.entry.prettify())\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a figure that shows the number of papers published in each month in the dataset!\n",
    "\n",
    "There are many ways you can do this: you can create a dataframe, you can use lists. You can write your own code to count the occurences in a month or use `np.unique` or `collections.Counter`. Google away if you need help.\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "The date and time when the paper was published is in the `<published>` tag. Create a date object with the year and the month of publishing, but set the day to 1.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "YMs = [datetime.date(year=int(pub.string[:4]),month=int(pub.string[5:7]),day=1) for pub in soup.find_all('published')]\n",
    "uniYMs, counts = np.unique(YMs, return_counts=True)\n",
    "plt.plot(uniYMs,counts,'o-')\n",
    "plt.ylabel(\"Number of covid papers\")\n",
    "plt.xticks(rotation=45);\n",
    "``` \n",
    "</p>\n",
    "</details>\n",
    "\n",
    "You can also look at the newest 1000 paper changing \"ascending\" to \"descending\" in the url. (arXiv.org does not recommend api search requests that return more than 1000 entries. If you are interested in accessing more, you can stich together the results of multiple requests, or you can look into the bulk data api.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print out the title of the paper that has the most co-authors!\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "Count the number of `<author>` tags contained within the `<entry>` tags.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "max_auth=0\n",
    "\n",
    "for entry in soup.find_all('entry'):\n",
    "    auth = len(entry.find_all('author'))\n",
    "    if auth>max_auth:\n",
    "        max_auth = auth\n",
    "        title = entry.title.string\n",
    "print(title)\n",
    "print('number of co-authors:', max_auth)\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  13 Planets\n",
    "\n",
    "Download the table containing info about planets compiled for you by NASA from here: https://nssdc.gsfc.nasa.gov/planetary/factsheet/\n",
    "Parse the table using BeautifulSoup and store the data in a pandas dataframe.\n",
    "\n",
    "It's generally a good idea to download and process the data in separate cells, so you don't repeat the download unnecessarily.\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "* The table is contained in the `<table>` tag. The table rows are inside `<tr>` tags, the table cells are represented by `<td>` tags.\n",
    "* The column names are in the first `<tr>` tag. Iterate over the rest of the `<tr>` tags to fetch the rows, the first `<td>` is the row index, the rest is the data.\n",
    "* Collect the table data as a list of lists, convert it to a dataframe, and rename the colunms and rows.  \n",
    "* You can use slices such as `soup.find_all(...)[1:]` to exclude tags as needed.\n",
    "   \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "webpage = urllib.request.urlopen(\"https://nssdc.gsfc.nasa.gov/planetary/factsheet/\")\n",
    "soup = BeautifulSoup(webpage,\"lxml\")   \n",
    " \n",
    "colnames = []\n",
    "for td in soup.table.tr.find_all('td')[1:]:\n",
    "    colnames.append(td.text.strip())\n",
    "colnames\n",
    "\n",
    "rownames = []\n",
    "rows =[]\n",
    "for tr in soup.table.find_all('tr')[1:-1]:\n",
    "    rownames.append(tr.td.text.strip())\n",
    "    rows.append([])\n",
    "    for td in tr.find_all('td')[1:]:\n",
    "        rows[-1].append(td.text.strip())\n",
    "\n",
    "df=pd.DataFrame(rows)\n",
    "df.columns=colnames\n",
    "df.set_index(pd.Index(rownames),inplace=True)\n",
    "df\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For good measure, create a plot showing the relationship between mean temperature and distance from the Sun.\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "You can directly use matplotlib's `plt.plot()`. Or you can use pandas plot, but `df.plot(...)` plots columns not rows, one solition is to transpose your dataframe using `df.T`.\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "df.T.plot(kind='scatter',x=\"Distance from Sun (106 km)\", y=\"Mean Temperature (C)\")\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14 Planets part 2\n",
    "\n",
    "Now do the same thing using `pd.read_html()` (check out the documentation for details).\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "Note that `pd.read_html()` returns a list of dataframes, one for each table in the html file.\n",
    "    \n",
    "One option to drop the rows that are labelled `nan` with\n",
    "```python\n",
    "df = df.loc[df.index.dropna(),:] \n",
    "```\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_html(\"https://nssdc.gsfc.nasa.gov/planetary/factsheet/\")[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "df = pd.read_html(\"https://nssdc.gsfc.nasa.gov/planetary/factsheet/\")[0]\n",
    "\n",
    "df.set_index(0, inplace=True)\n",
    "df.columns = df.iloc[0]\n",
    "df = df.loc[df.index.dropna(),:]\n",
    "df.index.name=None\n",
    "df.columns.name = None\n",
    "\n",
    "df\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why don't we always use pandas?\n",
    "* To learn about html and its parsing\n",
    "* Not all data from website comes from tables\n",
    "* Not all html is clean enough and `pd.html_read()` might not work as expected\n",
    "* etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15 Christmas\n",
    "\n",
    "Write a function `isitChristmas()` that prints no if it is not Christmas and yes if it is by scrapping the \n",
    "https://isitchristmas.today/ website.\n",
    "\n",
    "**There is a catch:**<br>\n",
    "When your browser sends a request to a server to download a page it also sends additional information about itself in the header of the request. Some websites block requests that do not have such information, and would not allow your script to access the contents. Luckily you can easily fake this by using\n",
    "```python\n",
    "#construct a request with the additional header\n",
    "req = urllib.request.Request(\"https://isitchristmas.today/\",headers={'User-Agent':'Mozilla/5.0'})\n",
    "#send the request and download the site\n",
    "webpage = urllib.request.urlopen(req)\n",
    "```\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "Visit the website with your browser and open the source code to see what tag is used to display the answer.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "def isitChristmas():\n",
    "    req = urllib.request.Request(\"https://isitchristmas.today/\",headers={'User-Agent':'Mozilla/5.0'})\n",
    "    webpage = urllib.request.urlopen(req)\n",
    "    soup = BeautifulSoup(webpage,\"lxml\") \n",
    "    \n",
    "    \n",
    "    if soup.html.body.h2.string==\"No!\":\n",
    "        print(\"no\")\n",
    "    else:\n",
    "        print(\"yes\")\n",
    "    \n",
    "    return\n",
    "\n",
    "isitChristmas()\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>\n",
    "\n",
    "Of course you could have done this a lot easier using datetime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16 GIFs\n",
    "\n",
    "Write a function that takes a string containing an URL of a website as input and returns the number of `gif` images on the website.\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "Look for `<img>` tags and their `src` attribute in the HTML files.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "def gifcounter(url):\n",
    "    webpage = urllib.request.urlopen(url,headers={'User-Agent':'Mozilla/5.0'})\n",
    "    soup = BeautifulSoup(webpage,\"lxml\") \n",
    "    gifcount = 0\n",
    "    for img in soup.find_all(\"img\"):\n",
    "        if img['src'][-3:]=='gif':\n",
    "            gifcount+=1\n",
    "        \n",
    "    return gifcount\n",
    "\n",
    "# or\n",
    "def gifcounter(url):\n",
    "    webpage = urllib.request.urlopen(url)\n",
    "    soup = BeautifulSoup(webpage,\"lxml\")\n",
    "        \n",
    "    return len(soup.find_all(lambda tag: tag.name=='img' and tag['src'][-3:]=='gif'))\n",
    "    \n",
    "print(gifcounter(\"http://posfaim.web.elte.hu/example.html\"))\n",
    "print(gifcounter(\"http://google.com\"))\n",
    "print(gifcounter(\"https://index.hu/velemeny/jegyzet/folio/\"))\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17 The gray lady\n",
    "\n",
    "Scrape the front page of the New York Times and retrieve the article titles.\n",
    "\n",
    "It is not necessarily a trival task to extract information robustly from a website. There is a lot of html and other code in the file that you download that is responsible for the visuals of the website, and this code is not meant to be read by a human directly. You can try to figure out a method yourself, but clicking on the hint reveals a possible approach.\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "* Each article title is a link represented as the `<a>` tag in html\n",
    "* There are many other links on the webpage, but article title links have an attribute `data-story` and the value of this attribute always starts with `\"nyt://article\"`\n",
    "    \n",
    "Write a function that takes a tag as input, returns `True` if it matches the above criteria, and `False` if it does not. Use this function together with `find_all` to identify all titles.\n",
    "\n",
    "(The above criteria is not entirely accurate, it will count some article descriptions as titles. To get more better results you can extend the criteria to only consider `<a>` tags that have at contain at least one `<h3>` tag. The last `<h3>` tag contains the title.)\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "req = urllib.request.Request(\"https://nytimes.com\",headers={'User-Agent':'Mozilla/5.0'})\n",
    "webpage = urllib.request.urlopen(req)\n",
    "soup = BeautifulSoup(webpage,\"lxml\")\n",
    "\n",
    "#solution 1\n",
    "def is_article(tag):\n",
    "    if tag.name=='a' and tag.get('data-story'):\n",
    "        if 'nyt://article' in tag['data-story']:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "titles = []\n",
    "for story in soup.find_all(is_article):\n",
    "        titles.append(story.text)\n",
    "titles[:3]\n",
    "    \n",
    "#solution 2\n",
    "def is_article(tag):\n",
    "    if tag.name=='a' and tag.get('data-story'):\n",
    "        if 'nyt://article' in tag['data-story']:\n",
    "            if tag.find('h3'):\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "titles = []\n",
    "for story in soup.find_all(is_article):\n",
    "        titles.append(story.find_all('h3')[-1].text)\n",
    "titles[:3]\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pick a word and calculate the percentage of the articles that contain this word in the title, ignore capitalization of the letters.\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "Convert the word and the title to lower case using the `lower()` string method.\n",
    "    \n",
    "You can use\n",
    "```python\n",
    "   if word in title:\n",
    "       ...\n",
    "```\n",
    "but this will also include cases when a word is contained inside a longer word, e.g., \"is\" is inside \"misreable\". You can get a more accurate count using regex expressions (check out `\\b`).\n",
    "\n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "#solution 1\n",
    "word = 'Biden'\n",
    "n = 0\n",
    "for t in titles:\n",
    "    if word.lower() in t.lower():\n",
    "        n+=1\n",
    "        print(t)\n",
    "print(word,\"count:\",n,\"ratio:\", n/len(titles))  \n",
    "    \n",
    "#solution 2\n",
    "import re\n",
    "word = \"Biden\"\n",
    "n = 0\n",
    "for t in titles:\n",
    "    if re.search(r\"\\b\"+word.lower()+r\"\\b\",t.lower()):\n",
    "        n+=1\n",
    "print(n,n/len(titles))\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18 Wiki\n",
    "\n",
    "Write a function that takes an English wikipedia article as input and returns a list of wikipedia articles that are linked by the page. For example, `\"Dog\"` has links to `\"Domesticated\"`, `\"Wolf\"` and many others.\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "Look for tags that\n",
    "* are links `<a>`,\n",
    "* have an attribute `href`\n",
    "* the value of `href` starts with `\"/wiki/\"` (you can use the `startswith()` string method),\n",
    "* and value of `href` does not contain the character `\":\"` (if it does contain `\":\"`, it links to a category or a media file, not an article).\n",
    "\n",
    "An article might be linked twice! Only collect unique article names.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "def iswikilink(tag):\n",
    "    if tag.name=='a' and tag.get('href'):\n",
    "        if tag['href'].startswith('/wiki/') and \":\" not in tag['href']:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def wikilinks(page):\n",
    "    webpage = urllib.request.urlopen(\"https://en.wikipedia.org/wiki/\"+page)\n",
    "    soup = BeautifulSoup(webpage,\"lxml\")\n",
    "    \n",
    "    links = []\n",
    "    for link in soup.find_all(iswikilink):\n",
    "        article = link['href'][6:]\n",
    "        if article not in links: \n",
    "            links.append(article)\n",
    "    return links\n",
    "\n",
    "wikilinks(\"Dog\")\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many articles are one click away from \"Ja-Da\"? How many articles are two clicks away from \"Ja-Da\"?\n",
    "\n",
    "<details><summary><u>Hint</u></summary>\n",
    "<p>\n",
    "\n",
    "* Get the list of articles one click away using the function you have just written.\n",
    "* Apply the same function to each article in this list and collect all articles two steps away.\n",
    "* Make sure you only count each article once.\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary><u>Solution.</u></summary>\n",
    "<p>\n",
    "    \n",
    "```python\n",
    "onestep = wikilinks(\"Ja-Da\")\n",
    "twostep = []\n",
    "for word in onestep:\n",
    "    twostep = twostep + wikilinks(word)\n",
    "len(onestep),len(set(twostep))\n",
    "```\n",
    "    \n",
    "</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final problem\n",
    "\n",
    "Solve either Part I, Option 1 or Option 2. In Part II, plan your final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Part I -- Option 1\n",
    "\n",
    "Write a function that returns two lists conataining the authors and the titles of the books on the New York Times best seller list.\n",
    "* Download and parse the page from here: https://www.nytimes.com/books/best-sellers/combined-print-and-e-book-fiction/\n",
    "* The book titles are encased in a tag with attribute itemprop=\"name\"\n",
    "* The authors are in a tag itemprop=\"author\"\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part I -- Option 2\n",
    "\n",
    "Predicting exchange rates is a difficult problem, so it totally makes sense to get help from any possible source. Look for correlations between movements of celestial objects and currency exchange rates!\n",
    "\n",
    " `weatherapi.com` has an astronomy api that can provide the illuminated portion of the moon for each day. It ranges from 0% (new moon) to 100% (full moon).\n",
    "\n",
    "* Download the moon illumination for the last 100 days.\n",
    "* Download all exchange rates for the last 100 days.\n",
    "* Find the currency that has the highest correlation with the moon phases (illuminated portion of the moon).\n",
    "* Create a plot that compares the moon illumination time series to this exchange rate time series (use [this type](https://matplotlib.org/devdocs/gallery/subplots_axes_and_figures/two_scales.html) of plot).\n",
    "\n",
    "Hint:\n",
    "* You download the exchange rates as a dictionary; you can collect the dictionaries corresponding to each day in a list `L`; and this list can be easily converted to a dataframe with\n",
    "```python\n",
    "df_rates= pd.DataFrame(L),\n",
    "```\n",
    "where the rows represent the days and the columns the different exchange rates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II\n",
    "\n",
    "Plan your final project! Check the requirement of the final project on moodle, and write a short paragraph describing your plans. Pick something that interests you and/or is useful for your research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
